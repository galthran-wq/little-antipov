max_seq_length: 2048 # Choose any! We auto support RoPE Scaling internally!
dtype: float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit: false # Use 4bit quantization to reduce memory usage. Can be False.
fourbit_model: "unsloth/Meta-Llama-3.1-8B-Instruct" # Default model
token: "..."
r: 64
use_rslora: false
training_args:
  seed: 42
  num_train_epochs: 5
  logging_steps: 30
chat_template: "llama-3.1"
conversations_paths: 
- "./data/some_channel/conversations.jsonl"
run_name: "llama-3.1-8b-some_channel-r=64-5epochs"
system_path: system.txt
